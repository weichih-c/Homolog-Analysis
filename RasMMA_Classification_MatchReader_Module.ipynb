{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as NaN\n",
    "import re\n",
    "import math\n",
    "\n",
    "%run CollectForestInfo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used for classification result reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate profile's maximum score in each family\n",
    "def profileScoreInEachFamily(profileScoreInFamilyBTs):\n",
    "    profileFamilyScores = dict()\n",
    "    for proc, familyBTScores in profileScoreInFamilyBTs.items():\n",
    "\n",
    "        profileFamilyScores[proc] = dict()\n",
    "        for familyName, profileBTScores in familyBTScores.items():\n",
    "            if familyName not in profileFamilyScores[proc].keys():\n",
    "                profileFamilyScores[proc][familyName] = (0,0,0)\n",
    "\n",
    "            for profileBTScore in profileBTScores:\n",
    "                maxScore, maxGSALen, maxScoreModel = profileFamilyScores[proc][familyName]\n",
    "                score, gsaLen, modelLen = profileBTScore\n",
    "                if float(score) > float(maxScore):\n",
    "                    profileFamilyScores[proc][familyName] = (score, gsaLen, modelLen)\n",
    "                elif float(score) == float(maxScore):\n",
    "                    if gsaLen > maxGSALen:\n",
    "                        profileFamilyScores[proc][familyName] = (score, gsaLen, modelLen)\n",
    "    return profileFamilyScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createScoreBox(result_data_dir, ignoreFamilys, weight_dict):\n",
    "    scoreBox = pd.DataFrame()\n",
    "    for resPkl in os.listdir(result_data_dir):\n",
    "        truth = resPkl.split('_')[1].split('.')[0]\n",
    "        with open(result_data_dir + resPkl, 'rb') as fHandle:\n",
    "            result = pickle.load(fHandle)\n",
    "            fHandle.close()\n",
    "            \n",
    "        processList = result.keys()\n",
    "        profileScoreInFamilyBTs = dict()\n",
    "        \n",
    "        # calculate profile's score with each behavior tree\n",
    "        # record with family name\n",
    "        for proc in processList:\n",
    "            familyBTScores = dict()\n",
    "            BTNames = result[proc]\n",
    "            for bt in BTNames:\n",
    "                famName = bt.split('_')[0]\n",
    "                if famName in ignoreFamilys: continue # skip unwanted family\n",
    "                trName = bt.split('_')[2]\n",
    "                gsaLen, commSeq = result[proc][bt]\n",
    "                modelWeight, modelLength, modelMemberCount = weight_dict[famName][trName]\n",
    "                \n",
    "                # filter trees\n",
    "                if modelLength > 10 and modelMemberCount > 2:\n",
    "                    if famName not in familyBTScores.keys():\n",
    "                        familyBTScores[famName] = list()\n",
    "                        \n",
    "                    profileInBT = gsaLen/modelLength # calculate gsa ratio score for bt\n",
    "                    familyBTScores[famName].append((profileInBT, gsaLen, modelLength))\n",
    "            profileScoreInFamilyBTs[proc] = familyBTScores\n",
    "            \n",
    "        # calculate profile's maximum score in each family\n",
    "        profileFamilyScores = profileScoreInEachFamily(profileScoreInFamilyBTs)\n",
    "\n",
    "        # record profile score of family to scorebox\n",
    "        for proc, familyScores in profileFamilyScores.items():\n",
    "            for famName, scoreInfo in familyScores.items():\n",
    "                score, gsaLen, modelLen = scoreInfo\n",
    "                score = (math.floor(score*100)) / 100\n",
    "                outputSentence = str(gsaLen) +'/'+ str(modelLen) + '=' + str(score)\n",
    "                scoreBox.loc[proc, famName+' Score'] = outputSentence\n",
    "    return scoreBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assignGroundTruth(scoreBox, sample_truth_dict):\n",
    "    myScoreBox = scoreBox.copy()\n",
    "    for proc in myScoreBox.index:\n",
    "        shaName = proc.split('_')[0]\n",
    "        myScoreBox.loc[proc, 'GroundTruthLabel'] = truth_dict[shaName]\n",
    "    return myScoreBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTruthScore(processScores, groundTruth):\n",
    "    for pair in processScores:\n",
    "        famName, score = pair\n",
    "        if groundTruth == famName:\n",
    "            return score\n",
    "    return None # should not touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exactMatch(threshold, processScores, groundTruth):\n",
    "    maxPair = processScores[0]\n",
    "    maxScoreFamily, maxScore = maxPair\n",
    "    \n",
    "    if maxScore < threshold:\n",
    "        return 3\n",
    "    else:\n",
    "        # if there are many equal maximum score, check whether truth in there.\n",
    "        multi_max = set()\n",
    "        for pair in processScores:\n",
    "            famName, score = pair\n",
    "            if score == maxScore:\n",
    "                multi_max.add(famName)\n",
    "#             else:\n",
    "#                 break\n",
    "        if groundTruth in multi_max:\n",
    "            return 1 # match\n",
    "        else:\n",
    "            return 2 # mismatch\n",
    "\n",
    "def effectiveMatch(threshold, processScores, groundTruth):\n",
    "    maxPair = processScores[0]\n",
    "    maxScoreFamily, maxScore = maxPair\n",
    "    \n",
    "    # prepare the prospective candidate set\n",
    "    candidateSet = set()\n",
    "    for pair in processScores:\n",
    "        famName = pair[0]\n",
    "        score = pair[1]\n",
    "        if score >= threshold:\n",
    "            candidateSet.add(famName)\n",
    "            \n",
    "    if len(candidateSet) == 0:\n",
    "        return 3\n",
    "    else:\n",
    "        if groundTruth in candidateSet: return 1\n",
    "        else: return 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMatchResult(scoreBox, threshold, matchMethod):\n",
    "    sample_scores = dict()\n",
    "    sample_truth = dict()\n",
    "    for procName, row in scoreBox.iterrows():\n",
    "        sampleName = procName.split('_')[0]\n",
    "        truthLabel = row['GroundTruthLabel']\n",
    "        if sampleName not in sample_scores.keys():\n",
    "            sample_scores[sampleName] = []\n",
    "            sample_truth[sampleName] = truthLabel\n",
    "\n",
    "        processScoreList = []\n",
    "        for col in range(0,28):\n",
    "            s_in_fam = float(row[col].split('=')[1])\n",
    "            famName = scoreBox.columns[col].split(' ')[0]\n",
    "            scorePair = (famName, s_in_fam)\n",
    "            processScoreList.append(scorePair)\n",
    "        sample_scores[sampleName].extend(processScoreList)\n",
    "\n",
    "    sample_result = dict()\n",
    "    matchResultTable = pd.DataFrame(columns = ['MatchType', 'PredictFamily', 'GroundTruth'])\n",
    "    for key, val in sample_scores.items():\n",
    "        truth = sample_truth[key]\n",
    "        processScores = sorted(val, key=lambda x:x[1], reverse=True)\n",
    "        maxPair = processScores[0]\n",
    "        maxScoreFamily, maxScore = maxPair\n",
    "        truthScore = getTruthScore(processScores, truth)\n",
    "\n",
    "        # there are 'Exact-Match' and 'Effective-Match' method.\n",
    "        # result: 1-match, 2-mismatch, 3-undecided\n",
    "        if matchMethod == 'Effective_Match':\n",
    "            matchResult = effectiveMatch(threshold, processScores, truth)\n",
    "        elif matchMethod == 'Exact_Match':\n",
    "            matchResult = exactMatch(threshold, processScores, truth)\n",
    "            \n",
    "        if matchResult == 1:\n",
    "            matchResultTable.loc[key, 'MatchType'] = 'Match'\n",
    "            matchResultTable.loc[key, 'PredictFamily'] = truth\n",
    "            matchResultTable.loc[key, 'GroundTruth'] = truth\n",
    "            matchResultTable.loc[key, 'MaxScore'] = maxScore\n",
    "            matchResultTable.loc[key, 'TruthScore'] = truthScore\n",
    "        else:\n",
    "            matchResultTable.loc[key, 'PredictFamily'] = maxScoreFamily\n",
    "            matchResultTable.loc[key, 'GroundTruth'] = truth\n",
    "            matchResultTable.loc[key, 'MaxScore'] = maxScore\n",
    "            matchResultTable.loc[key, 'TruthScore'] = truthScore\n",
    "            if matchResult == 2:\n",
    "                matchResultTable.loc[key, 'MatchType'] = 'Mismatch'\n",
    "            else:\n",
    "                matchResultTable.loc[key, 'MatchType'] = 'Undecided'\n",
    "\n",
    "    return matchResultTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readModelFiles(modelBaseDirectory):\n",
    "    base_dir = modelBaseDirectory\n",
    "    familyDirs = [base_dir+f+'/' for f in os.listdir(base_dir)]\n",
    "\n",
    "    weight_dict = dict()\n",
    "    ignoreFamilys = set()\n",
    "    for fam in familyDirs:\n",
    "        pickleDir = fam + 'pickle/'\n",
    "        tag = pickleDir.split('/')[-3]\n",
    "        interPkl = pickleDir + tag + \"_intermediate.pickle\"\n",
    "        residualPkl = pickleDir + tag + \"_residual.pickle\"\n",
    "        forestInfo = CollectForestInfo(interPkl,\n",
    "                               residualPkl,\n",
    "                               True) # one pickle is a forest\n",
    "\n",
    "        forestMemberCount = forestInfo.getForestMemberCount()\n",
    "        weight_dict[tag.split(\"_\")[0]] = dict()\n",
    "        for treeName in forestInfo.getTreeRootNameList():\n",
    "            labelName = tag+'_'+treeName\n",
    "            memberCount = len(forestInfo.getTreeMembers(treeName))\n",
    "            repSeq = forestInfo.getRepAPISeq(treeName)\n",
    "            weight_dict[tag.split(\"_\")[0]][treeName] = (memberCount/forestMemberCount,\n",
    "                                                        len(repSeq), memberCount)\n",
    "\n",
    "    for fName, trs in weight_dict.items():\n",
    "        save = False\n",
    "        for tr, info in trs.items():\n",
    "            if info[1] > 10 and info[2] > 2:\n",
    "                save = True\n",
    "                break\n",
    "        if not save:\n",
    "            ignoreFamilys.add(fName)\n",
    "\n",
    "    print(\"=== Finish building model ===\")\n",
    "\n",
    "    return ignoreFamilys, weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used for Write the Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getProcessMaxScore(dataframeRow, truthLabel):\n",
    "    maxScore = 0\n",
    "    outputScore = \"\"\n",
    "    for colName in dataframeRow.keys():\n",
    "        colVal = dataframeRow[colName]\n",
    "        score = float(colVal.split('=')[1])\n",
    "        gsaLen = int(colVal.split('/')[0])\n",
    "        if score > maxScore:\n",
    "            maxScore = score\n",
    "            outputScore = colVal\n",
    "        elif score == maxScore:\n",
    "            colName = colName.split(' ')[0]\n",
    "            if colName == truthLabel:\n",
    "                outputScore = colVal\n",
    "            else:\n",
    "                if gsaLen > int(outputScore.split('/')[0]):\n",
    "                    outputScore = colVal\n",
    "    return outputScore\n",
    "def getProcessTruthScore(dataframeRow, truthLabel):\n",
    "    for colName in dataframeRow.keys():\n",
    "        colVal = dataframeRow[colName]\n",
    "        shortColName = colName.split(' ')[0]\n",
    "        if shortColName == truthLabel:\n",
    "            return colVal\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Confusion Matrix and Do Evaluation\n",
    "##### Calculate 'Precision', 'Recall', and 'F1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluationMethodCalculate(matchResultTable):\n",
    "    family_confusion = dict()\n",
    "    for family in matchResultTable['GroundTruth'].values:\n",
    "        if family not in family_confusion.keys():\n",
    "            family_confusion[family] = {'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "\n",
    "    ## create confusion matrix for each family\n",
    "    for sample, row in matchResultTable.iterrows():\n",
    "        predictLabel = row['PredictFamily']\n",
    "        truthLabel = row['GroundTruth']\n",
    "        matchType = row['MatchType']\n",
    "\n",
    "        if matchType == 'Undecided':\n",
    "            family_confusion[truthLabel]['FN'] += 1\n",
    "        else:\n",
    "            if predictLabel == truthLabel:\n",
    "                family_confusion[truthLabel]['TP'] += 1\n",
    "            else:\n",
    "                family_confusion[truthLabel]['FN'] += 1\n",
    "                family_confusion[predictLabel]['FP'] += 1\n",
    "\n",
    "        for famName in family_confusion.keys():\n",
    "            if famName != truthLabel and famName != predictLabel:\n",
    "                family_confusion[famName]['TN'] += 1\n",
    "\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    fs = []\n",
    "    for famName in sorted(family_confusion.keys()):\n",
    "        matrix = family_confusion[famName]\n",
    "        tp = matrix['TP']\n",
    "        tn = matrix['TN']\n",
    "        fp = matrix['FP']\n",
    "        fn = matrix['FN']\n",
    "        population = tp+tn+fp+fn\n",
    "\n",
    "        if (tp+fp) != 0:\n",
    "            precision = tp/(tp+fp)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        recall = tp/(tp+fn)\n",
    "        if recall!=0 and precision!=0:\n",
    "            f1 = 2 / ((1/recall) + (1/precision))\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "        recalls.append(\"{0:.2f}\".format(recall))\n",
    "        precisions.append(\"{0:.2f}\".format(precision))\n",
    "        fs.append(\"{0:.2f}\".format(f1))\n",
    "\n",
    "    ### Create a dataframe table (dashboard)\n",
    "    evaTable = pd.DataFrame(columns=['Precision', 'Recall', 'F1'])\n",
    "    families = sorted(family_confusion.keys())\n",
    "    for idx in range(len(recalls)):\n",
    "        evaTable.loc[families[idx]] = (precisions[idx], recalls[idx], fs[idx])\n",
    "\n",
    "#     print(evaTable.shape)\n",
    "    return evaTable, precisions, recalls, fs, family_confusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
